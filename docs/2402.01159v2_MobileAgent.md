# Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception

[Text Extracted from PDF 2402.01159v2.pdf]

Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception

Zhu Yan, et al.

Abstract
The rapid advancement of Large Multi-modal Models (LMMs) has opened new possibilities for autonomous agents on mobile devices. This paper introduces Mobile-Agent, a multi-modal agent designed to operate mobile devices through visual perception and action execution. Unlike existing agents that rely on structured metadata (like XML or HTML), Mobile-Agent perceives the device screen directly as an image, allowing it to interact with any application regardless of its underlying technology. By combining visual understanding with reasoning, Mobile-Agent can perform complex tasks such as navigating social media, searching for information, and executing multi-step workflows. We evaluate Mobile-Agent on a variety of mobile tasks and demonstrate its superior performance and adaptability compared to text-based agents.

... [Content Truncated]
