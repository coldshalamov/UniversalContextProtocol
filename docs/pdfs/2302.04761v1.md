# Toolformer: Language Models Can Teach Themselves to Use Tools
**Source:** 2302.04761v1.pdf

## Abstract
Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.

## 1 Introduction
Large language models achieve impressive zero- and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent capabilities (Wei et al., 2022). However, all of these models have several inherent limitations that can at best be partially addressed by further scaling. These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).

A simple way to overcome these limitations of today’s language models is to give them the ability to use external tools such as search engines, calculators, or calendars. However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs. Therefore, we propose Toolformer, a model that learns to use tools in a novel way, which fulfills the following desiderata:
* The use of tools should be learned in a self-supervised way without requiring large amounts of human annotations.
* The LM should not lose any of its generality and should be able to decide for itself when and how to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to specific tasks.

Our approach for achieving these goals is based on the recent idea of using large LMs with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and Schütze, 2021b; Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, we finetune the LM itself on the API calls that it considers useful. As illustrated in Figure 1, through this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which tool to use when and how.

As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset that was used to pretrain a model in the first place. This ensures that the model does not lose any of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model (Brown et al., 2020) and several other baselines on various tasks.

## 2 Approach
Our aim is to equip a language model M with the ability to use different tools by means of API calls. We require that inputs and outputs for each API can be represented as text sequences. This allows seamless insertion of API calls into any given text, using special tokens to mark the start and end of each such call.

(See PDF for technical details on Sampling API Calls, Executing API Calls, Filtering API Calls, and Model Finetuning.)

## 3 Tools
We explore a variety of tools to address different shortcomings of regular LMs. The only constraints we impose on these tools is that (i) both their inputs and outputs can be represented as text sequences, and (ii) we can obtain a few demonstrations of their intended use. Concretely, we explore the following five tools: a question answering system, a Wikipedia search engine, a calculator, a calendar, and a machine translation system.

* **Question Answering:** A question answering system based on another LM (Atlas) that can answer simple factoid questions.
* **Calculator:** A calculator that can perform simple numeric calculations (four basic arithmetic operations).
* **Wikipedia Search:** A search engine that, given a search term, returns short text snippets from Wikipedia.
* **Machine Translation:** A machine translation system (NLLB) that can translate a phrase from any language into English.
* **Calendar:** A calendar API that returns the current date without taking any input.

## Figures and Tables
* **Figure 1:** Exemplary predictions of Toolformer. The model autonomously decides to call different APIs (QA, calculator, machine translation, Wikipedia search) to obtain information useful for completing a piece of text.
* **Figure 2:** Key steps in our approach: Sample API calls, Execute API calls, Filter API calls, Finetune LM.
* **Figure 3:** An exemplary prompt P(x) used to generate API calls for the question answering tool.
* **Figure 4:** Average performance on LAMA, math benchmarks, and QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach.
* **Table 1:** Examples of inputs and outputs for all APIs used.
* **Table 2:** Number of examples with API calls in C* for different values of filtering threshold.
* **Table 3:** Results on subsets of LAMA (SQuAD, Google-RE, T-REx).
* **Table 4:** Results for benchmarks requiring mathematical reasoning (ASDiv, SVAMP, MAWPS).
* **Table 5:** Results for various question answering datasets (WebQS, NQ, TriviaQA).
* **Table 6:** Results on MLQA for multilingual question answering.
* **Table 7:** Results for temporal datasets (TempLAMA, Dateset).
* **Table 8:** Perplexities of different models on WikiText and CCNet.
