# Gorilla: Large Language Model Connected with Massive APIs
**Source:** 2305.15334v1.pdf

## Abstract
Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of-the-art LLMs such as GPT-4, owing to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model’s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla’s code, model, data, and demo are available at https://gorilla.cs.berkeley.edu

## 1 Introduction
Recent advances in large language models (LLMs) have enabled significant new capabilities including natural dialogue, mathematical reasoning, and program synthesis. However, despite these advances, LLMs are still fundamentally limited by the information they can store in a fixed set of weights and the things they can compute using a static computation graph and limited context. Furthermore, as the world changes, LLMs require retraining to update their knowledge and reasoning capabilities.

By empowering LLMs to use tools, we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks. However, much of the prior work integrating tools into LLMs considered a small well documented set of APIs that can be easily injected into the prompt.

(See PDF for full Introduction.)

## 2 Related Work
* **Large Language Models:** Discusses recent strides in LLMs (LLaMA, Alpaca, Vicuna) and their application in program synthesis.
* **Tool Usage:** Discusses Toolformer and other tool-using models. Gorilla aims to explore a vast array of tools (API calls) in an open-ended fashion.
* **LLMs for Program Synthesis:** Harnessing LLMs for program synthesis and code generation.

## 3 Methodology
In this section, we describe APIBench, a comprehensive benchmark constructed from TorchHub, TensorHub, and HuggingFace API Model Cards. We begin by outlining the process of collecting the API dataset and how we generated instruction-answer pairs. We then introduce Gorilla, a novel training paradigm with a information-retriever incorporated into the training and inference pipelines. Finally, we present our AST tree matching evaluation metric.

### 3.1 Dataset Collection
To collect the dataset, we meticulously recorded all online model cards for HuggingFace’s “The Model Hub”, PyTorch Hub, and TensorFlow Hub Models.
* **API Documentation:** We consider top 20 models from each domain in HuggingFace (925 models total after filtering), 95 models from Torch Hub, and 696 APIs from TensorHub.
* **Instruction Generation:** Guided by the self-instruct paradigm, we employed GPT-4 to generate synthetic instruction data.

### 3.2 Gorilla
Our model Gorilla, is retrieve-aware finetuned LLaMA-7B model, specifically for API calls. We convert this to a user-agent chat-style conversation, where each data-point is a conversation with one round each for the user and the agent.
* **API Call with Constraints:** API calls often come with constraints (parameter size, accuracy bounds). Gorilla is trained to understand these.
* **Retriever-Aware training:** For training with retriever, the instruction-tuned dataset also has an additional "Use this API documentation for reference: <retrieved_API_doc_JSON>" appended to the user prompt.

## 4 Evaluation
We carried out an array of experiments on our collected dataset, benchmarking our model Gorilla with other models, and exploring how different retrieval methods may impact the performance of the model in making API calls.
* **Baselines:** GPT-4, GPT-3.5-turbo, Claude-v1, LLaMA-7B.
* **Retrievers:** Zero-shot (no retriever), BM25, GPT-Index, Oracle retriever.
* **AST Accuracy on API call:** We use AST sub-tree matching to identify which API in our dataset is the LLM calling and verify arguments.

## Figures and Tables
* **Figure 1:** Examples of API calls generated by GPT-4, Claude, and Gorilla. Gorilla identifies the task correctly and suggests a fully-qualified API call without hallucination.
* **Figure 2:** Accuracy (vs) hallucination in four settings (0-shot, BM25, GPT-Retriever, Oracle). Gorilla improves accuracy while reducing hallucination.
* **Figure 3:** Gorilla: A system for enabling LLMs to interact with APIs.
* **Figure 4:** AST Sub-Tree Matching to evaluate API calls.
* **Figure 5:** Accuracy with GPT-retriever. Gorilla outperforms on Torch Hub and Hugging-Face.
* **Figure 6:** Gorilla's retriever-aware training enables it to react to changes in the APIs.
* **Figure 8:** Example of the Dataset (User prompt, Assistant response with domain, api_call, api_provider, explanation, code).
* **Table 1:** Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs.
* **Table 2:** Comparison of retrieval techniques.
* **Table 3:** Evaluating LLMs on constraint-aware API invocations.
